{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c5d08680","cell_type":"markdown","source":"# **Reti Neurali** ","metadata":{}},{"id":"52a5f30f","cell_type":"markdown","source":"## **Esercizio 1: Download e pre-processamento dei dati.**\n\nScaricare e pre-processare i dati per il successivo addestramento del modello. \n\nIl dataset che utilizzeremo sarà CIFAR10, scaricabile dalla libreria `tensorflow.keras.datasets`","metadata":{}},{"id":"42e9791d","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\n\n# Download dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\n# Stampare le shape dei dati\nprint('dati di train:',x_train.shape)\nprint('dati di test:',x_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T06:56:34.076683Z","iopub.execute_input":"2025-05-27T06:56:34.077140Z","iopub.status.idle":"2025-05-27T06:56:37.004744Z","shell.execute_reply.started":"2025-05-27T06:56:34.077112Z","shell.execute_reply":"2025-05-27T06:56:37.003873Z"}},"outputs":[{"name":"stdout","text":"dati di train: (50000, 32, 32, 3)\ndati di test: (10000, 32, 32, 3)\n","output_type":"stream"}],"execution_count":5},{"id":"4d6a0ca6","cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Pre-processamento dei dati\nscaler=StandardScaler()\n\nx_train = x_train.reshape(x_train.shape[0], -1)  # Flatten the images\nx_test = x_test.reshape(x_test.shape[0], -1)  # Flatten the images\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:21:27.086916Z","iopub.execute_input":"2025-05-27T07:21:27.087286Z","iopub.status.idle":"2025-05-27T07:21:30.122248Z","shell.execute_reply.started":"2025-05-27T07:21:27.087262Z","shell.execute_reply":"2025-05-27T07:21:30.120904Z"}},"outputs":[],"execution_count":17},{"id":"5103ad7f","cell_type":"markdown","source":"## **Esercizio 2: Creare modello MLP**\n\nPer creare il modello MLP utilizziamo l' oggetto `MLPClassifier` dal modulo `sklearn.neural_networks`. Questo è un oggetto molto complesso che prevede la possibilità di specificare tanti parametri, permettendoci una personalizzazione molto dettagliata. Vediamo di seguito gli argomenti principali:\n\n- `hidden_layer_sizes`: rappresenta la struttura dell' MLP, sotto forma di una tupla. La tupla deve essere composta da numeri interi, ogni numero indica il numero di neuroni presenti nel rispettivo layer.\n\nEsempio: \n\n`hidden_layer_sizes` = `(100)`\n\ncreerà un solo layer con 100 neuroni\n\n`hidden_layer_sizes` = `(100, 50)`\n\ncreerà due layer, il primo con 100 neuroni, il secondo invece con 50.\n\n- `max_iter`: massimo numero di iterazioni per raggiungere la convergenza. \n\n- `activation`: indica quale funzione di attivazione utilizzare, valori possibili sono `'relu'`, `'logistic'`, `'tanh'` and `'identity'`.\n\n- `solver`: indica quale algoritmo di ottimizzazione utilizzare, valori possibili sono `'adam'`, `'sgd'` and `'lbfgs'`.\n\n- `learning_rate_init`: valore iniziale del learning rate.\n\n- `verbose`: valore booleano che, se impostato su `True`, stampa l' output di ogni iterazione di training. Molto utile per monitorare il training.\n\n- `random_state`: fissa il seed della randomizzazione.\n","metadata":{}},{"id":"b0f857d0","cell_type":"markdown","source":"Per iniziare creiamo un MLP molto basilare, alleniamolo e testiamone le performance. Come parametri utilizzeremo:\n\n- `hidden_layer_sizes` = `(100)`\n\n- `max_iter` = `20`\n\n- `random_state` = `42`","metadata":{}},{"id":"4650894a","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP \nMLP=MLPClassifier(hidden_layer_sizes = (100),max_iter = 20,random_state = 42)\n\n# Allenare MLP\nMLP.fit(x_train,y_train)\n\n# Valutare MLP\npredictions_MLP=MLP.predict(x_test)\n\n# Stampare l' accuratezza\naccuracy_MLP = accuracy_score(y_test, predictions_MLP)\nprint('Accuracy:',accuracy_MLP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:12:04.334558Z","iopub.execute_input":"2025-05-27T07:12:04.334915Z","iopub.status.idle":"2025-05-27T07:13:02.018762Z","shell.execute_reply.started":"2025-05-27T07:12:04.334889Z","shell.execute_reply":"2025-05-27T07:13:02.017962Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.5057\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"id":"6171280e","cell_type":"markdown","source":"## **Esercizio 2.1: Aumentiamo i parametri del nostro modello**\n\nProviamo adesso ad aumentare i dettagli del nostro modello, modificando o aggiungendo i parametri sopra specificati. ","metadata":{}},{"id":"f35afa68","cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creare MLP con più strati e altre specifiche\nMLP_1=MLPClassifier(hidden_layer_sizes = (100,50),max_iter = 1,random_state = 42,activation='relu',solver='sgd',warm_start=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T07:55:37.735408Z","iopub.execute_input":"2025-05-27T07:55:37.735730Z","iopub.status.idle":"2025-05-27T07:55:37.741385Z","shell.execute_reply.started":"2025-05-27T07:55:37.735707Z","shell.execute_reply":"2025-05-27T07:55:37.740359Z"}},"outputs":[],"execution_count":33},{"id":"226b1681","cell_type":"markdown","source":"## **Esercizio 3: Implementare manualmente l' algoritmo di early stopping.**\n\nL' algoritmo di early stopping ci permette di terminare anticipatamente l' allenamento di un modello nel caso in cui questo raggiunga la convergenza. Supponiamo infatti che il nostro modello raggiunga un certo livello di accuratezza e che non riesca a migliorare oltre quel livello. Questo significa che il modello, da quel momento in poi, non sta più apprendendo nuove informazioni, per cui le successive iterazioni sono superflue, ed inoltre rischiano di essere dannose, spingendo il modello verso l' overfitting. \n\nL' early stopping verifica ad ogni iterazione che l' accuratezza del modello sia incrementata di una certa tolleranza. Se questa tolleranza non viene superata per un certo numero di epoche, allora possiamo decidere di stoppare l' allenamento in quanto il modello ha raggiunto la convergenza.\n\n**N.B: per applicare early stopping è necessario specificare i seguenti parametri dell' MLP:**\n\n- `warm_start`=`True` in modo che il training proceda dallo stato attuale del modello e non dall' inizializzazione.\n\n- `max_iter`=`1` in modo che il modello venga allenato per una sola epoca. Per l' early stopping infatti dovremo gestire manualmente il numero di iterazioni.","metadata":{}},{"id":"8ee1709d","cell_type":"code","source":"import numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n#divisione del train set in train set e validation set\nvalidation_size = int(0.2 * len(x_train))\nx_val = x_train[:validation_size]\ny_val = y_train[:validation_size]\nx_train_split = x_train[validation_size:]\ny_train_split = y_train[validation_size:]\n\nn_total_epochs = 100  \npatience = 10         \ntolerance = 1e-4      \n\nbest_test_accuracy = 0.0\nepochs_without_improvement = 0\n\nfor epoch in range(n_total_epochs):\n    MLP_1.fit(x_train, y_train)\n\n    y_val_pred = MLP_1.predict(x_val)\n    val_accuracy = accuracy_score(y_val, y_val_pred)\n\n    print(f\"Epoca {epoch + 1}/{n_total_epochs}, Accuracy: {val_accuracy:.4f}\")\n\n    if val_accuracy >best_test_accuracy -tolerance:\n        best_test_accuracy = val_accuracy\n        epochs_without_improvement = 0\n    else:\n        epochs_without_improvement += 1\n\n    if epochs_without_improvement >= patience:\n        print(\"Arresto anticipato\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T08:32:20.027163Z","iopub.execute_input":"2025-05-27T08:32:20.027531Z","iopub.status.idle":"2025-05-27T08:36:59.086564Z","shell.execute_reply.started":"2025-05-27T08:32:20.027505Z","shell.execute_reply":"2025-05-27T08:36:59.085355Z"}},"outputs":[{"name":"stdout","text":"Epoca 1/100, Accuracy: 0.8828\nEpoca 2/100, Accuracy: 0.8835\nEpoca 3/100, Accuracy: 0.8836\nEpoca 4/100, Accuracy: 0.8854\nEpoca 5/100, Accuracy: 0.8838\nEpoca 6/100, Accuracy: 0.8865\nEpoca 7/100, Accuracy: 0.8851\nEpoca 8/100, Accuracy: 0.8880\nEpoca 9/100, Accuracy: 0.8865\nEpoca 10/100, Accuracy: 0.8883\nEpoca 11/100, Accuracy: 0.8866\nEpoca 12/100, Accuracy: 0.8894\nEpoca 13/100, Accuracy: 0.8900\nEpoca 14/100, Accuracy: 0.8889\nEpoca 15/100, Accuracy: 0.8898\nEpoca 16/100, Accuracy: 0.8911\nEpoca 17/100, Accuracy: 0.8893\nEpoca 18/100, Accuracy: 0.8903\nEpoca 19/100, Accuracy: 0.8932\nEpoca 20/100, Accuracy: 0.8896\nEpoca 21/100, Accuracy: 0.8916\nEpoca 22/100, Accuracy: 0.8892\nEpoca 23/100, Accuracy: 0.8933\nEpoca 24/100, Accuracy: 0.8930\nEpoca 25/100, Accuracy: 0.8935\nEpoca 26/100, Accuracy: 0.8931\nEpoca 27/100, Accuracy: 0.8944\nEpoca 28/100, Accuracy: 0.8969\nEpoca 29/100, Accuracy: 0.8968\nEpoca 30/100, Accuracy: 0.8976\nEpoca 31/100, Accuracy: 0.8984\nEpoca 32/100, Accuracy: 0.8996\nEpoca 33/100, Accuracy: 0.8935\nEpoca 34/100, Accuracy: 0.8974\nEpoca 35/100, Accuracy: 0.8993\nEpoca 36/100, Accuracy: 0.8965\nEpoca 37/100, Accuracy: 0.8963\nEpoca 38/100, Accuracy: 0.9004\nEpoca 39/100, Accuracy: 0.8957\nEpoca 40/100, Accuracy: 0.8989\nEpoca 41/100, Accuracy: 0.8979\nEpoca 42/100, Accuracy: 0.8996\nEpoca 43/100, Accuracy: 0.8958\nEpoca 44/100, Accuracy: 0.8994\nEpoca 45/100, Accuracy: 0.8979\nEpoca 46/100, Accuracy: 0.9022\nEpoca 47/100, Accuracy: 0.8996\nEpoca 48/100, Accuracy: 0.9030\nEpoca 49/100, Accuracy: 0.9016\nEpoca 50/100, Accuracy: 0.9000\nEpoca 51/100, Accuracy: 0.9047\nEpoca 52/100, Accuracy: 0.9027\nEpoca 53/100, Accuracy: 0.9027\nEpoca 54/100, Accuracy: 0.8977\nEpoca 55/100, Accuracy: 0.9050\nEpoca 56/100, Accuracy: 0.9008\nEpoca 57/100, Accuracy: 0.9021\nEpoca 58/100, Accuracy: 0.8976\nEpoca 59/100, Accuracy: 0.9012\nEpoca 60/100, Accuracy: 0.9048\nEpoca 61/100, Accuracy: 0.9013\nEpoca 62/100, Accuracy: 0.9047\nEpoca 63/100, Accuracy: 0.9001\nEpoca 64/100, Accuracy: 0.9096\nEpoca 65/100, Accuracy: 0.9029\nEpoca 66/100, Accuracy: 0.9071\nEpoca 67/100, Accuracy: 0.9076\nEpoca 68/100, Accuracy: 0.9091\nEpoca 69/100, Accuracy: 0.9022\nEpoca 70/100, Accuracy: 0.9084\nEpoca 71/100, Accuracy: 0.9085\nEpoca 72/100, Accuracy: 0.9100\nEpoca 73/100, Accuracy: 0.9130\nEpoca 74/100, Accuracy: 0.9088\nEpoca 75/100, Accuracy: 0.9099\nEpoca 76/100, Accuracy: 0.9148\nEpoca 77/100, Accuracy: 0.9132\nEpoca 78/100, Accuracy: 0.9141\nEpoca 79/100, Accuracy: 0.9180\nEpoca 80/100, Accuracy: 0.9163\nEpoca 81/100, Accuracy: 0.9135\nEpoca 82/100, Accuracy: 0.9155\nEpoca 83/100, Accuracy: 0.9137\nEpoca 84/100, Accuracy: 0.9152\nEpoca 85/100, Accuracy: 0.9119\nEpoca 86/100, Accuracy: 0.9104\nEpoca 87/100, Accuracy: 0.9106\nEpoca 88/100, Accuracy: 0.9072\nEpoca 89/100, Accuracy: 0.9120\nArresto anticipato\n","output_type":"stream"}],"execution_count":46}]}